{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare dataset\n",
    "\n",
    "# Dataset CIFAR-10\n",
    "# downloaded from: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "# introduced in: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DATA_DIR = 'dataset/'\n",
    "EXTRACTED_DATA_DIR = 'cifar-10-batches-py/'\n",
    "\n",
    "def download_extract_if_necessary(dest_dir, data_url, expected_tarfile, expected_extracted_file):\n",
    "    \"\"\"\n",
    "    Download (if necessary) and extract (if necessary) a file\n",
    "    in tar.gz format (if necessary) to a given directory.\n",
    "    \n",
    "    Both arguments 'expected_*' help to avoid unnecessary download or extraction.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    \n",
    "    dest_filename = os.path.join(dest_dir, expected_tarfile)\n",
    "    if not os.path.exists(dest_filename):\n",
    "        print('Downloading data from %s...' % data_url)\n",
    "        dest_filename, _ = urllib.request.urlretrieve(data_url, dest_filename)\n",
    "        print('Download finished')\n",
    "    \n",
    "    if not os.path.exists(os.path.join(dest_dir, expected_extracted_file)):\n",
    "        print('Extracting archive...')\n",
    "        with tarfile.open(dest_filename, \"r:gz\") as tar:\n",
    "            tar.extractall(dest_dir)\n",
    "    \n",
    "    print('Extracted file(s) ready in directory: %s' % dest_dir)\n",
    "    \n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "download_extract_if_necessary(\n",
    "    dest_dir=DATA_DIR,\n",
    "    data_url='https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "    expected_tarfile='cifar-10-python.tar.gz',\n",
    "    expected_extracted_file=EXTRACTED_DATA_DIR\n",
    ")\n",
    "\n",
    "batch_train_dicts = [\n",
    "    unpickle(os.path.join(DATA_DIR, EXTRACTED_DATA_DIR, ('data_batch_%d' % i)))\n",
    "    for i in range(1, 6)\n",
    "]\n",
    "batch_test_dict = unpickle(os.path.join(DATA_DIR, EXTRACTED_DATA_DIR, 'test_batch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training set batches together, for simplified processing\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "BATCH_CNT = 5\n",
    "TRAIN_HEIGHT = 50000  # = BATCH_SIZE * BATCH_CNT = sum([batch_dicts[0][b'data'].shape[i] for i in range(5)])\n",
    "TRAIN_WIDTH = batch_train_dicts[0][b'data'].shape[1]\n",
    "\n",
    "# join batches together\n",
    "X_train = np.empty((TRAIN_HEIGHT, TRAIN_WIDTH), dtype='uint8')\n",
    "y_train = np.empty(TRAIN_HEIGHT, dtype='uint8')\n",
    "for i in range(0, BATCH_CNT):\n",
    "    X_train[(i * BATCH_SIZE):((i + 1) * BATCH_SIZE)] = batch_train_dicts[i][b'data']\n",
    "    y_train[(i * BATCH_SIZE):((i + 1) * BATCH_SIZE)] = batch_train_dicts[i][b'labels']\n",
    "\n",
    "X_test = batch_test_dict[b'data']\n",
    "y_test = np.asarray(batch_test_dict[b'labels'], dtype='uint8')\n",
    "\n",
    "print('Full training set size: %d' % len(X_train))\n",
    "print('Full test set size: %d' % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert images to RGB format\n",
    "\n",
    "def cifar_to_rgb_dataset(imgs):\n",
    "    \"\"\"\n",
    "    Change format from CIFAR-like to matplotlib-like of all given images \n",
    "    \n",
    "    :param imgs_cifar: an array of images represented by list of 3072 consecutive pixel values:\n",
    "        first all red, then green, then blue; row-wise\n",
    "    :return: an array of shape (..., 32, 32, 3), with values of type 'float32'\n",
    "    \"\"\"\n",
    "    img_3d = np.reshape(imgs, (-1, 3, 32, 32))\n",
    "    img_rgb = np.transpose(img_3d, (0, 2, 3, 1))\n",
    "    # scale values to [0, 1] interval:\n",
    "    return np.asarray(img_rgb, dtype='float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plot images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "CLASS_CNT = 10  # = np.unique(test_labels)\n",
    "CLASS_SAMPLE_SIZE = 10  # class images sample size\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "for cls in range(CLASS_CNT):\n",
    "    # select class images\n",
    "    X_class = X_train[y_train == cls]\n",
    "    # choose 10 random images and convert to RGB format\n",
    "    rnd_indices = np.random.choice(len(X_class), CLASS_SAMPLE_SIZE, replace=False)\n",
    "    X_cls = cifar_to_rgb_dataset(X_class[rnd_indices])\n",
    "    # plot them\n",
    "    for x, img in enumerate(X_cls):\n",
    "        fig.add_subplot(CLASS_CNT, CLASS_SAMPLE_SIZE, cls * CLASS_SAMPLE_SIZE + x + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract HOG features\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "\n",
    "def rgb_to_hog(img):\n",
    "    img_gray = color.rgb2gray(img)\n",
    "    return hog(img_gray, block_norm='L2-Hys', visualise=False)\n",
    "\n",
    "def rgb_to_hog_dataset(imgs):\n",
    "    \"\"\" Calculate HOG for all images in dataset \"\"\"\n",
    "    result = list()\n",
    "    for img in imgs:\n",
    "        result.append(rgb_to_hog(img))\n",
    "    return np.asarray(result, dtype='float32')\n",
    "\n",
    "# Extract HOG features, starting from CIFAR format\n",
    "def cifar_to_hog(imgs):\n",
    "    return rgb_to_hog_dataset(cifar_to_rgb_dataset(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Shallow classifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from shutil import rmtree\n",
    "from tempfile import mkdtemp\n",
    "from time import time\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "hogs = FunctionTransformer(cifar_to_hog)\n",
    "pipe = Pipeline([('hog', hogs), ('norm', StandardScaler()), ('svc', SVC())], memory=cachedir)\n",
    "\n",
    "Cs = np.logspace(0, 1, 2)\n",
    "grid_params = [{'svc__kernel': ['rbf'], 'svc__C': [1., 10.]},\n",
    "               {'svc__kernel': ['linear'], 'svc__C': [1.]}]\n",
    "\n",
    "clf = GridSearchCV(pipe, grid_params, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit estimator using a smaller subset of images\n",
    "\n",
    "# TODO: set appropriate value / use full dataset:\n",
    "SUBSET_SIZE = 100\n",
    "\n",
    "def choose_random_subset(X, y, subset_size):\n",
    "    indices = np.random.permutation(len(X))[:subset_size]\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "X, y = choose_random_subset(X_train, y_train, SUBSET_SIZE)\n",
    "\n",
    "print('Reduced training set size: %d' % len(X))\n",
    "print()\n",
    "print('Fitting ...')\n",
    "start = time()\n",
    "clf.fit(X, y)\n",
    "end = time()\n",
    "\n",
    "print('Fitting done.')\n",
    "print('Time elapsed: %0.03fs' % (end - start,))\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search summary\n",
    "\n",
    "print('Best parameters set found on training set:')\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print('Grid scores on training set:')\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "print('Score on test set:')\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visual features\n",
    "\n",
    "# Inception v3 model, trained on ImageNet data\n",
    "# source: http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "MODEL_DIR = 'inception/'\n",
    "MODEL_FILE = 'classify_image_graph_def.pb'\n",
    "\n",
    "# Download inception model\n",
    "download_extract_if_necessary(\n",
    "    dest_dir=MODEL_DIR,\n",
    "    data_url='http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz',\n",
    "    expected_tarfile='inception-2015-12-05.tgz',\n",
    "    expected_extracted_file=MODEL_FILE\n",
    ")\n",
    "\n",
    "# Load the model graph (with pretrained weights)\n",
    "def create_graph():\n",
    "  \"\"\" Creates a graph from saved GraphDef file. \"\"\"\n",
    "  with tf.gfile.FastGFile(os.path.join(MODEL_DIR, MODEL_FILE), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "create_graph()\n",
    "sess = tf.Session()\n",
    "bottleneck = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "input_tensor_name = 'DecodeJpeg:0'\n",
    "resized_image_tensor_name = 'ResizeBilinear:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods for visual features extraction from CIFAR10 images with Inception model\n",
    "\n",
    "def extract_cnn_codes(rgb_img):\n",
    "    \"\"\"\n",
    "    :param rgb_img: an array (?, ?, 3) with pixels in range [0,255] (*NOT* [0,1]).\n",
    "    :return: an array (2048,) of calculated CNN codes\n",
    "    \"\"\"\n",
    "    cnn_codes = sess.run(bottleneck, {input_tensor_name: rgb_img})\n",
    "    return np.squeeze(cnn_codes)\n",
    "\n",
    "def cifar_to_cnn_codes_dataset(imgs):\n",
    "    \"\"\"\n",
    "    :param imgs: an array (?, 3072) of type 'float32' with pixels in range [0,1]\n",
    "    :return: an array (?, 2048) of calculated CNN codes\n",
    "    \"\"\"\n",
    "    rgb_imgs = cifar_to_rgb_dataset(imgs) * 255  # rescale to values in range [0,255]\n",
    "    result = list()\n",
    "    for img in rgb_imgs:\n",
    "        result.append(extract_cnn_codes(img))\n",
    "    return np.asarray(result, dtype='float32')\n",
    "\n",
    "def load_or_compute_cnn_codes(X, codes_file):\n",
    "    \"\"\"\n",
    "    Computes CNN codes for dataset X and saves them to file 'codes_file'.\n",
    "    If file already exists, use it.\n",
    "    \n",
    "    :param X: an array of images in CIFAR format\n",
    "    :param codes_file: filepath (String)\n",
    "    :return: an array (?, 2048) of calculated CNN codes\n",
    "    \"\"\"\n",
    "    codes_loaded = False\n",
    "    if os.path.exists(codes_file):\n",
    "        # Load codes from file\n",
    "        try:\n",
    "            X_codes = np.load(codes_file)\n",
    "            # check if dataset matches codes length\n",
    "            # (weak condition of data integrity, but it's enough here)\n",
    "            if len(X) == len(X_codes):\n",
    "                codes_loaded = True\n",
    "                print('CNN codes loaded successfully from file %s' % codes_file)\n",
    "            else:\n",
    "                print('Invalid codes present in file, replacing with new ones...')\n",
    "            \n",
    "        except (IOError, ValueError):\n",
    "            print('Error during codes loading')\n",
    "    \n",
    "    if not codes_loaded:\n",
    "        # Compute codes\n",
    "        print('Start computing CNN codes...')\n",
    "        start = time()\n",
    "        X_codes = cifar_to_cnn_codes_dataset(X)\n",
    "        end = time()\n",
    "\n",
    "        print('Computing done.')\n",
    "        print('  Time elapsed: %0.02fs.' % (end - start,))\n",
    "        print('  Average time: %0.02fs/image' % ((end - start) / len(X)))\n",
    "        \n",
    "        # Save codes to file\n",
    "        np.save(codes_file, X_codes)\n",
    "        print('Codes saved succesfully to file %s' % codes_file)\n",
    "        print()\n",
    "        \n",
    "    return X_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute CNN codes in batches (to save checkpoints)\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "NUM_FEATURES = 2048  # length of bottleneck layer\n",
    "\n",
    "def compute_codes_in_batches(X, b_size, filename_pattern):\n",
    "    \"\"\"\n",
    "    Compute CNN codes for images X, in batches of size 'b_size'.\n",
    "    The purpose of this method is checkpointing each batch on disk.\n",
    "    Subsequent batches will be saved to files according to 'filename_pattern',\n",
    "    which is a string with one integer to fill (batch_index).\n",
    "    \"\"\"\n",
    "\n",
    "    X_codes = np.empty((len(X), NUM_FEATURES), dtype='float32')\n",
    "\n",
    "    for i in range(ceil(len(X) / float(b_size))):\n",
    "        X_codes[i * b_size: (i + 1) * b_size] = load_or_compute_cnn_codes(\n",
    "                                                    X[i * b_size: (i + 1) * b_size],\n",
    "                                                    filename_pattern % i\n",
    "                                                )\n",
    "    print('All codes computed!')\n",
    "    return X_codes\n",
    "\n",
    "    \n",
    "if not os.path.exists('codes/'):\n",
    "    os.makedirs('codes/')\n",
    "\n",
    "X_codes = compute_codes_in_batches(X_train, 1000, 'codes/codes_train_%d.npy')\n",
    "X_codes_test = compute_codes_in_batches(X_test, 100, 'codes/codes_test_%d.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN codes in 2 dimensions\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Normalize data first\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_codes_norm = normalizer.fit_transform(X_codes)\n",
    "X_codes_norm_test = normalizer.transform(X_codes_test)\n",
    "\n",
    "# Reduce dimensionality using first PCA, then t-SNE\n",
    "\n",
    "# TODO: plot more than 1000 examples\n",
    "X_vis, y_vis = choose_random_subset(X_codes_norm, y_train, 100)\n",
    "\n",
    "reduction = make_pipeline(PCA(10), TSNE(2))\n",
    "%time X_vis_2d = reduction.fit_transform(X_vis)\n",
    "\n",
    "# Plot\n",
    "y_normalized = y_vis.astype('float32') / 9.  # scale to range [0,1]\n",
    "colors = plt.cm.rainbow(y_normalized)\n",
    "plt.scatter(X_vis_2d[:, 0], X_vis_2d[:, 1], c=colors, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save and load models from disk\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def save_model(filename, model):\n",
    "    joblib.dump(filename, model)\n",
    "    \n",
    "def load_model(filename):\n",
    "    return joblib.load(filename)\n",
    "\n",
    "# TODO: train_and_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7. Train SVM model on top of CNN codes\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "grid_params = [{'svc__kernel': ['rbf'], 'svc__C': [1., 10.]},\n",
    "               {'svc__kernel': ['linear'], 'svc__C': [1.]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(), grid_params, cv=3, n_jobs=-1)\n",
    "\n",
    "X, y = choose_random_subset(X_codes_norm, y_train, 5000)\n",
    "%time clf.fit(X, y)\n",
    "# TODO: print GridSearch summary\n",
    "save_model('svm_cv_7.pkl', clf)\n",
    "\n",
    "# Grid search summary\n",
    "\n",
    "print('Best parameters set found on training set:')\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print('Grid scores on training set:')\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "print('Score on test set:')\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}