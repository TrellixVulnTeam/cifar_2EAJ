{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare dataset\n",
    "\n",
    "# Dataset CIFAR-10\n",
    "# downloaded from: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "# described in: Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "DATA_TARFILE = 'cifar-10-python.tar.gz'\n",
    "DATA_DIR = 'dataset'\n",
    "DATA_EXTRACTED_DIR = 'dataset/cifar-10-batches-py'\n",
    "\n",
    "def download_extract_dataset(dest_dir):\n",
    "    \"\"\" Download and extract CIFAR-10 dataset (if necessary) to a given directory. \"\"\"\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    \n",
    "    dest_filename = os.path.join(dest_dir, DATA_TARFILE)\n",
    "    if not os.path.exists(dest_filename):\n",
    "        print('Downloading data from %s...' % DATA_URL)\n",
    "        urllib.request.urlretrieve(DATA_URL, dest_filename)\n",
    "        print('Download finished')\n",
    "    \n",
    "    if not os.path.exists(DATA_EXTRACTED_DIR):\n",
    "        print('Extracting archive...')\n",
    "        with tarfile.open(dest_filename, \"r:gz\") as tar:\n",
    "            tar.extractall(DATA_DIR)\n",
    "    \n",
    "    print('Dataset ready in directory: %s' % DATA_EXTRACTED_DIR)\n",
    "    \n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "download_extract_dataset(DATA_DIR)\n",
    "\n",
    "batch_train_dicts = [unpickle(os.path.join(DATA_EXTRACTED_DIR, ('data_batch_%d' % i))) for i in range(1, 6)]\n",
    "batch_test_dict = unpickle(os.path.join(DATA_EXTRACTED_DIR, 'test_batch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join training set batches together, for simplified processing\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "BATCH_SIZE = 10000\n",
    "BATCH_CNT = 5\n",
    "TRAIN_HEIGHT = 50000  # = BATCH_SIZE * BATCH_CNT = sum([batch_dicts[0][b'data'].shape[i] for i in range(5)])\n",
    "TRAIN_WIDTH = batch_train_dicts[0][b'data'].shape[1]\n",
    "\n",
    "# join batches together\n",
    "X_train = np.empty((TRAIN_HEIGHT, TRAIN_WIDTH), dtype='uint8')\n",
    "y_train = np.empty(TRAIN_HEIGHT, dtype='uint8')\n",
    "for i in range(0, BATCH_CNT):\n",
    "    X_train[(i * BATCH_SIZE):((i + 1) * BATCH_SIZE)] = batch_train_dicts[i][b'data']\n",
    "    y_train[(i * BATCH_SIZE):((i + 1) * BATCH_SIZE)] = batch_train_dicts[i][b'labels']\n",
    "\n",
    "X_test = batch_test_dict[b'data']\n",
    "y_test = np.asarray(batch_test_dict[b'labels'], dtype='uint8')\n",
    "\n",
    "print('Full training set size: %d' % len(X_train))\n",
    "print('Full test set size: %d' % len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert images to RGB format\n",
    "\n",
    "def cifar_to_rgb_dataset(imgs):\n",
    "    \"\"\"\n",
    "    Change format from CIFAR-like to matplotlib-like of all given images \n",
    "    \n",
    "    :param imgs_cifar: an array of images represented by list of 3072 consecutive pixel values:\n",
    "        first all red, then green, then blue; row-wise\n",
    "    :return: an array of shape (..., 32, 32, 3), with values of type 'float32'\n",
    "    \"\"\"\n",
    "    img_3d = np.reshape(imgs, (-1, 3, 32, 32))\n",
    "    img_rgb = np.transpose(img_3d, (0, 2, 3, 1))\n",
    "    # scale values to [0, 1] interval:\n",
    "    return np.asarray(img_rgb, dtype='float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plot images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "CLASS_CNT = 10  # = np.unique(test_labels)\n",
    "CLASS_SAMPLE_SIZE = 10  # class images sample size\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "for cls in range(CLASS_CNT):\n",
    "    # select class images\n",
    "    X_class = X_train[y_train == cls]\n",
    "    # choose 10 random images and convert to RGB format\n",
    "    rnd_indices = np.random.choice(len(X_class), CLASS_SAMPLE_SIZE, replace=False)\n",
    "    X_cls = cifar_to_rgb_dataset(X_class[rnd_indices])\n",
    "    # plot them\n",
    "    for x, img in enumerate(X_cls):\n",
    "        fig.add_subplot(CLASS_CNT, CLASS_SAMPLE_SIZE, cls * CLASS_SAMPLE_SIZE + x + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract HOG features\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "\n",
    "def rgb_to_hog(img):\n",
    "    img_gray = color.rgb2gray(img)\n",
    "    return hog(img_gray, block_norm='L2-Hys', visualise=False)\n",
    "\n",
    "def rgb_to_hog_dataset(imgs):\n",
    "    \"\"\" Calculate HOG for all images in dataset \"\"\"\n",
    "    result = list()\n",
    "    for img in imgs:\n",
    "        result.append(rgb_to_hog(img))\n",
    "    return np.asarray(result, dtype='float32')\n",
    "\n",
    "# Extract HOG features, starting from CIFAR format\n",
    "def cifar_to_hog(imgs):\n",
    "    return rgb_to_hog_dataset(cifar_to_rgb_dataset(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Shallow classifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from shutil import rmtree\n",
    "from tempfile import mkdtemp\n",
    "from time import time\n",
    "\n",
    "cachedir = mkdtemp()\n",
    "hogs = FunctionTransformer(cifar_to_hog)\n",
    "pipe = Pipeline([('hog', hogs), ('norm', StandardScaler()), ('svc', SVC())], memory=cachedir)\n",
    "\n",
    "Cs = np.logspace(-1, 3, 5)\n",
    "grid_params = {'svc__kernel': ['rbf', 'linear'], 'svc__C': Cs}\n",
    "\n",
    "clf = GridSearchCV(pipe, grid_params, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit estimator using a smaller subset of images\n",
    "\n",
    "# TODO: set appropriate value / use full dataset:\n",
    "SUBSET_SIZE = 1000\n",
    "\n",
    "def choose_random_subset(X, y, subset_size):\n",
    "    indices = np.random.permutation(len(X))[:subset_size]\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "X, y = choose_random_subset(X_train, y_train, SUBSET_SIZE)\n",
    "\n",
    "print('Reduced training set size: %d' % len(X))\n",
    "print()\n",
    "print('Fitting ...')\n",
    "start = time()\n",
    " \n",
    "clf.fit(X, y)\n",
    " \n",
    "end = time()\n",
    "print('Fitting done.')\n",
    "print('Time elapsed: %0.03fs' % (end - start,))\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search summary\n",
    "\n",
    "print('Best parameters set found on training set:')\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print('Grid scores on training set:')\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "print('Score on test set:')\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
